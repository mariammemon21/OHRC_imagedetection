# -*- coding: utf-8 -*-
"""ISRO-OHRC.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1fPa8qlRwtXMfMYD6l85YfIDoJYwGli9F
"""

import cv2
from PIL import Image
from sklearn.cluster import BisectingKMeans
import numpy as np
import pandas as pd
import gc
from sklearn.cluster import KMeans
import os
import matplotlib.pyplot as plt
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
from sklearn.model_selection import train_test_split
import glob
from pprint import pprint
import math
from typing import Literal

def get_combined_df(img,ksize):
    combine = combined_transforms(img,ksize)
    df_white = pd.DataFrame(combine['white_coords'], columns = ['white_x','white_y','white_w','white_h','white_area'])
    df_black = pd.DataFrame(combine['black_coords'], columns = ['black_x','black_y','black_w','black_h','black_area'])
    return {
        'white_df' : df_white,
        'black_df' : df_black,
        'mask' : combine['mask'],
        'white_bounding_box' : combine['white_bounding_box'],
        'black_bounding_box' : combine['black_bounding_box']
    }

def slice_and_extract_bbox_data(img, slice_height, slice_width, ksize, x_offset = 0, y_offset = 0):
    h,w = img.shape[:2]
    data = []
    for i in range(0,h ,slice_height):
        for j in range(0,w ,slice_width):
            d = {
                'x_start' : j + x_offset,
                'y_start' : i + y_offset,
                'slice_height' :  min(slice_height,(h - i)),
                'slice_width' :  min(slice_width,(w - j)),
            }

            d.update(get_combined_df(img[i:i + slice_height, j:j+slice_width],ksize))
            data.append(d)
    return data


def get_slices_from_image_path(img_path, slice_height, slice_width, ksize, img_preprocess_func = lambda x,*args,**kwargs : x, img_preprocess_args = [], img_preprocess_kwargs = {}):
    img = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE)
    img = img_preprocess_func(img,*img_preprocess_args,**img_preprocess_kwargs)
    df = pd.DataFrame(slice_and_extract_bbox_data(img, slice_height, slice_width, ksize))
    df['img_path'] = img_path
    return df

def get_slices_from_binary_image_path(
    img_path,
    xml_path,
    slice_height,
    slice_width,
    ksize,
    img_preprocess_func = lambda x,*args,**kwargs : x,
    img_preprocess_args = [],
    img_preprocess_kwargs = {}
):
    img = read_pds_binary_image(img_path,'uint8').reshape(*extract_image_shape(xml_path))
    img = img_preprocess_func(img,*img_preprocess_args,**img_preprocess_kwargs)

    gc.collect()
    df = pd.DataFrame(slice_and_extract_bbox_data(img, slice_height, slice_width, ksize))
    df['img_path'] = img_path
    return df



def get_slices_from_image_path_list(
    img_paths,
    slice_height,
    slice_width,
    ksize,
    img_preprocess_func = lambda x,*args,**kwargs : x,
    img_preprocess_args = [],
    img_preprocess_kwargs = {}
):
    data = []
    for img_path in img_paths:
        df = get_slices_from_image_path(
            img_path,
            slice_height,
            slice_width,
            ksize,
            img_preprocess_func,
            img_preprocess_args,
            img_preprocess_kwargs,
        )
        data.append(df)
    df = pd.concat(data, axis = 0, ignore_index = True)
    return df

def get_slices_from_binary_image_path_list(
    img_paths,
    xml_paths,
    slice_height,
    slice_width,
    ksize,
    img_preprocess_func = lambda x,*args,**kwargs : x,
    img_preprocess_args = [],
    img_preprocess_kwargs = {}
):
    data = []
    for img_path in img_paths:
        df = get_slices_from_binary_image_path(
            img_path,
            slice_height,
            slice_width,
            ksize,
            img_preprocess_func,
            img_preprocess_args,
            img_preprocess_kwargs,
        )
        data.append(df)
    df = pd.concat(data, axis = 0, ignore_index = True)
    return df




from PIL import Image, ImageDraw
import numpy as np

def rotate_point(origin, point, angle):
    """
    Rotate a point around a given origin by angle.

    Parameters:
    - origin: tuple, (x, y) coordinates of the origin point
    - point: tuple, (x, y) coordinates of the point to rotate
    - angle: float, angle in degrees

    Returns:
    - rotated_point: tuple, the rotated point coordinates
    """
    ox, oy = origin
    px, py = point
    angle = np.radians(angle)
    qx = ox + np.cos(angle) * (px - ox) - np.sin(angle) * (py - oy)
    qy = oy + np.sin(angle) * (px - ox) + np.cos(angle) * (py - oy)
    return qx, qy

def extract_rotated_bbox(image_path, bbox, angle):
    """
    Extracts a rotated bounding box from an image.

    Parameters:
    - image_path: str, path to the image file
    - bbox: tuple, the bounding box of the region to extract (left, upper, right, lower)
    - angle: float, the angle to rotate the bounding box

    Returns:
    - rotated_region: Image object, the extracted and rotated region
    """
    image = Image.open(image_path)
    left, upper, right, lower = bbox

    # Define the center of the bounding box
    center = ((left + right) / 2, (upper + lower) / 2)

    # Define the corner points of the bounding box
    corners = [
        (left, upper),
        (right, upper),
        (right, lower),
        (left, lower)
    ]

    # Rotate each corner point around the center
    rotated_corners = [rotate_point(center, corner, angle) for corner in corners]

    # Find the bounding box of the rotated region
    rotated_left = min(x for x, y in rotated_corners)
    rotated_right = max(x for x, y in rotated_corners)
    rotated_upper = min(y for x, y in rotated_corners)
    rotated_lower = max(y for x, y in rotated_corners)

    # Crop the image to the new bounding box
    rotated_bbox = (rotated_left, rotated_upper, rotated_right, rotated_lower)
    cropped_image = image.crop(rotated_bbox)

    # Rotate the cropped image to get the final result
    rotated_region = cropped_image.rotate(-angle, expand=True)

    return rotated_region


def extract_slice(file_path, width, height, x = None, y = None, slice_width = None, slice_height = None):
    """
    file_path : Path of the image File
    width : width of the image
    height : height of the image
    x : x coordinate from where you want to slice
    y : y coordinate from where you want to slice
    slice_width : width of the slice
    slice_height : height of the slice
    """
    # Adjust slice_width and slice_height if they extend beyond image boundaries
    if x + slice_width > width:
        slice_width = width - x
    if y + slice_height > height:
        slice_height = height - y

    # Initialize the slice array
    slice_array = np.zeros((slice_height, slice_width), dtype=np.uint8)

    with open(file_path, 'rb') as f:
        for row in range(slice_height):
            # Calculate the position to seek in the file
            pos = (y + row) * width + x
            f.seek(pos)
            # Read the slice_width bytes for the current row
            row_data = f.read(slice_width)
            # Store the row data in the numpy array
            slice_array[row, :] = np.frombuffer(row_data, dtype=np.uint8)

    return slice_array

def read_pds_binary_image(file_path, data_type):
    """
    Read a PDS binary image from a file.

    Parameters:
    - file_path: str, path to the binary image file.
    - width: int, width of the image.
    - height: int, height of the image.
    - data_type: str, data type of the image (e.g., 'uint8', 'uint16', 'float32').

    Returns:
    - image: np.ndarray, the image read from the file.
    """
    # Map data type strings to NumPy data types
    data_type_map = {
        'uint8': np.uint8,
        'uint16': np.uint16,
        'float32': np.float32
    }

    if data_type not in data_type_map:
        raise ValueError(f"Unsupported data type: {data_type}")

    # Read the binary data from the file
    with open(file_path, 'rb') as file:
        binary_data = file.read()

    # Convert the binary data to a NumPy array
    image = np.frombuffer(binary_data, dtype=data_type_map[data_type])



    return image



def morphological_smoothing(image, kernel_size=(5, 5)):
    # Create a kernel for morphological operations
    kernel = np.ones(kernel_size, np.uint8)

    # Apply dilation followed by erosion (opening operation)
    dilated_image = cv2.dilate(image, kernel, iterations=1)
    smoothed_image = cv2.erode(dilated_image, kernel, iterations=1)

    return smoothed_image

def plot_images(img_mat, n_cols = 3, max_width = 15):
    h = len(img_mat)
    w = len(img_mat[0])
    total = h * w
    fig_x = max_width
    n_rows = total // n_cols + 1
    fig_y = (max_width / n_cols) * n_rows
    fig,ax = plt.subplots(nrows = n_rows, ncols = n_cols,figsize = (fig_x,fig_y))
    axes = iter(ax.flatten())
    for i in range(h):
        for j in range(w):
            a = next(axes)
            a.imshow(img_mat[i][j])
            a.axis('off')
            a.set_title(f'({i},{j})')
            a.margins(0)

    while True:
        try:
            a = next(axes)
            a.axis('off')
            a.margins(0)
        except StopIteration:
            break
    plt.subplots_adjust(wspace=0)
    plt.show()




def get_sliding_slices(img, slice_width = 1200, slice_height = 1200):
    slices = []
    h,w,*c = img.shape
    for i in range(0,h,slice_height):
        row = []
        for j in range(0,w,slice_width):
            row.append(img[i:i + slice_height,j : j + slice_width])
        slices.append(row)
    return slices,len(slices), len(slices[0])

def increase_contrast(image):
    """
    Increase the contrast of a grayscale image using CLAHE (Contrast Limited Adaptive Histogram Equalization).

    Parameters:
    - image: np.ndarray, input grayscale image.

    Returns:
    - contrast_image: np.ndarray, contrast-enhanced image.
    """
    # Create a CLAHE object (Arguments are optional)
    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))
    contrast_image = clahe.apply(image)

    return contrast_image

def canny_edge_detection(image, blur_ksize=(5, 5), blur_sigma=1.4, low_threshold=100, high_threshold=200):
    """
    Apply Canny edge detection to an image.

    Parameters:
    - image_path: str, path to the input image.
    - blur_ksize: tuple, kernel size for Gaussian blur.
    - blur_sigma: float, standard deviation for Gaussian blur.
    - low_threshold: int, lower threshold for Canny edge detection.
    - high_threshold: int, upper threshold for Canny edge detection.

    Returns:
    - edges: np.ndarray, edges detected in the image.
    """
    # Convert to grayscale
    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)

    # Apply Gaussian blur
    blurred_image = cv2.GaussianBlur(gray_image, blur_ksize, blur_sigma)

    # Apply Canny edge detection
    edges = cv2.Canny(blurred_image, low_threshold, high_threshold)

    return edges

def morphological_transform(original_img):
    kernel = np.ones(shape=(7,7), dtype=np.uint8)
    transformed_image = cv2.morphologyEx(cv2.morphologyEx(cv2.morphologyEx(original_img, cv2.MORPH_OPEN, kernel), cv2.MORPH_CLOSE, kernel), cv2.MORPH_GRADIENT, kernel)
#     binary_img = cv2.adaptiveThreshold(transformed_image, 1, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 7, 0.5)
    return transformed_image
#         cv2.imwrite('transformed.png', transformed_image)
#         cv2.imwrite('binary.png', binary_img)



def get_morphological_segmentation_mask(image,kernel):
    kernel = np.array(kernel, dtype = np.uint8)
    segmented_image = cv2.morphologyEx(cv2.morphologyEx(cv2.morphologyEx(image, cv2.MORPH_OPEN, kernel), cv2.MORPH_CLOSE, kernel), cv2.MORPH_GRADIENT, kernel)
    segmented_image = cv2.adaptiveThreshold(segmented_image, 1, cv2.ADAPTIVE_THRESH_GAUSSIAN_C, cv2.THRESH_BINARY, 7, 0.5)
    segmented_image = segmented_image*255
    return segmented_image


def circle_hough_transform(img,mask, resized_size = None):
    cimg = img.copy()
    mask = mask.copy()
    if resized_size is not None:
        mask = cv2.resize(mask,resized_size)
        cimg = cv2.resize(cimg,resized_size)

    circles = cv2.HoughCircles(mask,cv2.HOUGH_GRADIENT,1,50,param1=50,param2=35,
                              minRadius=0,maxRadius=200)
    circles = np.uint16(np.around(circles))
    for i in circles[0,:]:
        # draw the outer circle
        cimg = cv2.circle(cimg,(i[0],i[1]),i[2],(0,255,0),2)
        # draw the center of the circle
        cimg  = cv2.circle(cimg,(i[0],i[1]),2,(0,0,255),3)
    return cimg

def adaptive_intensity_scaling(img, epsilon = 0.0001,quantiles = [0.25,0.95]):
    img_reciprocated = img.astype(np.float64)
    min_val,max_val = np.quantile(img_reciprocated.flatten(),quantiles)
    img_reciprocated[img_reciprocated < min_val] = min_val
    img_reciprocated[img_reciprocated > max_val] = max_val
    img_reciprocated = (img_reciprocated - img_reciprocated.min()) / (img_reciprocated.max() - img_reciprocated.min() + epsilon)
    img_reciprocated = (img_reciprocated * 255).astype(np.uint8)
    return img_reciprocated

def reciprocation_transforms(img,quantiles = [0.15,0.5,0.75], masks = ['min','none','max']):
    img_reciprocated = img.astype(np.float64)
    img_reciprocated = (img_reciprocated - img_reciprocated.min()) / (img_reciprocated.max() - img_reciprocated.min() + 0.01)
    min_val,med_val,max_val = np.quantile(img_reciprocated.flatten(),quantiles)
    img_reciprocated[img_reciprocated < min_val] = max_val if masks[0] == 'max' else min_val
    if masks[1] != 'none':
        img_reciprocated[(img_reciprocated > min_val)  &  (img_reciprocated < max_val)] =  max_val if masks[1] == 'max' else (med_val if masks[1] == 'med' else min_val)
    img_reciprocated[img_reciprocated > max_val] = max_val =  max_val if masks[2] == 'max' else min_val
    img_reciprocated = (img_reciprocated - min_val) / (max_val - min_val + 0.001)

    img_reciprocated = (img_reciprocated * 255).astype(np.uint8)
    return img_reciprocated

def terneary_clustering(img, cluster_intensities = None):
    h,w = img.shape[:2]
    cluster = KMeans(n_clusters = 3,max_iter = 1000)
    labels = cluster.fit_predict(img.flatten().reshape(-1,1))
    if cluster_intensities:
        sorted_clusters = sorted(list(zip(range(cluster.cluster_centers_.shape[0]),cluster.cluster_centers_)),key = lambda x : x[1])
        for i,c in enumerate(cluster_intensities):
            cluster.cluster_centers_[sorted_clusters[i][0]] = sorted_clusters[i][1]
    compressed_image = recreate_image(cluster.cluster_centers_, labels, h, w).astype('uint8')
    return compressed_image

def detect_bound_islands(mask_image, image,ret_islands = False, thickness = 2, shade = 0):
    num_labels, labels_im = cv2.connectedComponents(mask_image)

    # Prepare the output image
    output_image = image.copy()
    print(num_labels)
    # Iterate through each component
    for label in range(1, num_labels):  # Start from 1 to exclude the background
        # Create a binary mask for the current label
        mask = labels_im == label
        # Find coordinates of pixels belonging to this label
        coords = np.column_stack(np.where(mask))
        # Find bounding box for the label
        x, y, w, h = cv2.boundingRect(coords)


        # Draw the bounding box on the original image
        output_image = cv2.rectangle(output_image, (y,x), (y + h, x + w), shade,thickness)

    if ret_islands:
        return output_image,labels_im
    return output_image

def draw_bounding_boxes_from_rects(
    img,
    bboxes,
    shade = 0,
    thickness = 2,
):
    output = img.copy()
    for stats in bboxes:
        x = stats[i, cv2.CC_STAT_LEFT]
        y = stats[i, cv2.CC_STAT_TOP]
        w = stats[i, cv2.CC_STAT_WIDTH]
        h = stats[i, cv2.CC_STAT_HEIGHT]
        area = stats[i, cv2.CC_STAT_AREA]
        output = cv2.rectangle(output,(x,y),(x + w, y + h),shade,thickness)
    return output

def detect_bound_islands_with_stats(
    mask_image,
    image,ret_image = True,
    ret_islands = False,
    thickness = 2,
    shade = 0,
    circularity_threshold = None,
    min_area_threshold = None,
    max_area_threshold = None,
):
    output = cv2.connectedComponentsWithStats(mask_image,)
    (numLabels, labels, stats, centroids) = output
    # Prepare the output imageoutput_image = img[0:1200,0:1200,0].copy()
    output_arr = []
    if (ret_image):
        output_image = image.copy()
        output_arr.append(output_image)
    for i in range(1, numLabels):
        # extract the connected component statistics and centroid for
        # the current label
        x = stats[i, cv2.CC_STAT_LEFT]
        y = stats[i, cv2.CC_STAT_TOP]
        w = stats[i, cv2.CC_STAT_WIDTH]
        h = stats[i, cv2.CC_STAT_HEIGHT]
        area = stats[i, cv2.CC_STAT_AREA]
        (cX, cY) = centroids[i]
        if min_area_threshold is not None and area < min_area_threshold:
            continue;
        if max_area_threshold is not None and area > max_area_threshold:
            continue
        if circularity_threshold is not None:
            circularity = calculate_circularity_of_boolean_mask((labels== i))
            if circularity < circularity_threshold:
                continue

        if (ret_image):
            output_image = cv2.rectangle(output_image, (x,y), (x+ w, y + h), shade,thickness)

    if ret_islands:
        output_arr.extend([stats,centroids,labels])
    return output_arr






def combined_transforms(
    img,
    adaptive_intensity_scaling_args = [],
    adaptive_intensity_scaling_kwargs = {'quantiles' : [0.05,0.95]},
    reciprocation_transforms_args = [],
    reciprocation_transforms_kwargs = {'quantiles' : [0.20,0.5,0.80], 'masks' : ['min','med','max']},
    median_blur_args = [],
    median_blur_kwargs = {'ksize' : 7},
    detect_bound_islands_with_stats_white_args = [],
    detect_bound_islands_with_stats_white_kwargs = {
        'ret_islands':True,'thickness':2
    },
    detect_bound_islands_with_stats_black_args = [],
    detect_bound_islands_with_stats_black_kwargs = {
        'ret_islands':True,'thickness':2
    },
):
    img = adaptive_intensity_scaling(img,*adaptive_intensity_scaling_args,**adaptive_intensity_scaling_kwargs)
    coarse_mask = reciprocation_transforms(img,*reciprocation_transforms_args, **reciprocation_transforms_kwargs)
    mask = cv2.medianBlur(coarse_mask,*median_blur_args,**median_blur_kwargs)
    white_bounding_box, white_coords, white_centroids = detect_bound_islands_with_stats(
        (mask == mask.max())*np.uint8(255), img,*detect_bound_islands_with_stats_white_args,**detect_bound_islands_with_stats_white_kwargs
    )
    black_bounding_box, black_coords, black_centroids = detect_bound_islands_with_stats(
        (mask == mask.min())*np.uint8(255), img,*detect_bound_islands_with_stats_black_args,**detect_bound_islands_with_stats_black_kwargs
    )
    return {
        'coarse_mask' : coarse_mask,
        'mask' : mask,
        'white_bounding_box' : white_bounding_box,
        'white_coords' : white_coords,
        'white_centroids' : white_centroids,
        'black_bounding_box' : black_bounding_box,
        'black_coords' : black_coords,
        'black_centroids' : black_centroids,
    }

def fit_ellipse(img,mask):
    contours, _ = cv2.findContours(mask, cv2.RETR_TREE, cv2.CHAIN_APPROX_SIMPLE)
    img_copy = img.copy()
    h,w = img_copy.shape[:2]
    img_copy = img_copy.reshape(h,w,-1) + np.array([[[0,0,0]]]).astype(np.uint8)

    for contour in contours:
        if len(contour) >= 5:  # Need at least 5 points to fit an ellipse
            ellipse = cv2.fitEllipse(contour)
            center, axes, angle = ellipse

            # Check if the dimensions of the ellipse are valid
            if axes[0] > 0 and axes[1] > 0:
                img_copy = cv2.ellipse(img_copy, ellipse, (0, 255, 0), 1)  # Draw the ellipse
    return img_copy

def show_img(img,figsize = (10,10)):
    plt.figure(figsize = (10,10))
    h,w = img.shape[:2]
    plt.imshow(img.reshape(h,w,-1) + [[[0,0,0]]])
    plt.axis('off')
    plt.show()

def slice_and_plot(img, slice_height, slice_width):
    img_slices,h,w = get_sliding_slices(img,slice_height,slice_width)
    img_slices_df = pd.DataFrame(img_slices)
    return h,w,img_slices_df,img_slices

def get_paths_by_re(base_directory, regular_expression):
    search_pattern = os.path.join(base_directory,regular_expression)
    # Use glob to find all files matching the pattern
    png_files = glob.glob(search_pattern, recursive=True)

    return png_files

def calculate_circularity_of_boolean_mask(boolean_mask):
    """
    Calculate the circularity of the connected component in a boolean mask using OpenCV.

    Parameters:
    - boolean_mask: np.array, a binary mask where the component is True.

    Returns:
    - float, the circularity of the component.
    """
    # Convert the boolean mask to uint8
    uint8_mask = (boolean_mask * 255).astype(np.uint8)

    # Find contours in the mask
    contours, _ = cv2.findContours(uint8_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if len(contours) == 0:
        return -1
    # Assuming a single connected component
    contour = contours[0]

    # Calculate area and perimeter
    area = cv2.contourArea(contour)
    perimeter = cv2.arcLength(contour, True)

    if perimeter == 0:
        return 0

    # Calculate circularity
    circularity = 4 * np.pi * (area / (perimeter ** 2))
    return circularity

def calculate_convexity(boolean_mask):
    """
    Calculate the convexity of the connected component in a boolean mask using OpenCV.

    Parameters:
    - boolean_mask: np.array, a binary mask where the component is True.

    Returns:
    - float, the convexity of the component.
    """
    # Convert the boolean mask to uint8
    uint8_mask = (boolean_mask * 255).astype(np.uint8)

    # Find contours in the mask
    contours, _ = cv2.findContours(uint8_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if len(contours) == 0:
        return -1

    # Assuming a single connected component
    contour = contours[0]

    # Calculate the perimeter of the contour
    perimeter = cv2.arcLength(contour, True)

    # Calculate the convex hull of the contour
    hull = cv2.convexHull(contour)

    # Calculate the perimeter of the convex hull
    hull_perimeter = cv2.arcLength(hull, True)

    if perimeter == 0:
        return float('inf')

    # Calculate convexity
    convexity = hull_perimeter / perimeter
    return convexity

def recreate_image_from_cluster_labels(codebook, labels, w, h):
    d = codebook.shape[1]
    image = np.zeros((w, h, d))
    label_idx = 0
    for i in range(w):
        for j in range(h):
            image[i, j] = codebook[labels[label_idx]]
            label_idx += 1
    return image
def terneary_clustering(image, cluster_intensities = None):
    """

    """
    h,w = image.shape[:2]
    cluster = KMeans(n_clusters = 3,max_iter = 1000)
    labels = cluster.fit_predict(image.flatten().reshape(-1,1))
    if cluster_intensities:
        sorted_clusters = sorted(list(zip(range(cluster.cluster_centers_.shape[0]),cluster.cluster_centers_)),key = lambda x : x[1])
        for i,c in enumerate(cluster_intensities):
            cluster.cluster_centers_[sorted_clusters[i][0]] = sorted_clusters[i][1]
    compressed_image = recreate_image_from_cluster_labels(cluster.cluster_centers_, labels, h, w).astype('uint8')
    return compressed_image

def terneary_clustering_thresholding(image, cluster_intensities = None):
    """

    """
    h,w = image.shape[:2]
    cluster = KMeans(n_clusters = 3,max_iter = 1000)
    labels = cluster.fit_predict(image.flatten().reshape(-1,1))
    if cluster_intensities:
        sorted_clusters = sorted(list(zip(range(cluster.cluster_centers_.shape[0]),cluster.cluster_centers_)),key = lambda x : x[1])
        for i,c in enumerate(cluster_intensities):
            cluster.cluster_centers_[sorted_clusters[i][0]] = sorted_clusters[i][1]
    compressed_image = recreate_image_from_cluster_labels(cluster.cluster_centers_, labels, h, w).astype('uint8')
    return compressed_image

def adaptive_intensity_scaling(
    image : np.ndarray[np.uint8],
    epsilon = 0.0001,
    quantiles : tuple[int,int] = (0.25,0.95)
) -> np.ndarray[np.uint8]:
    """
    Adaptively rescales the brightness of the image by performing min max scaling between the
    -quantiles[0]% quantile value (value which is greater than quantiles[0]% of all values present)
    and
    -quantiles[1]% quantile value (value which is greater than quantiles[1]% of all values present)
    epsilon is a small constant added to prevent zero division error when min max scaling
    """
    image_reciprocated = image.astype(np.float64)
    min_val,max_val = np.quantile(image_reciprocated.flatten(),quantiles)
    image_reciprocated[image_reciprocated < min_val] = min_val
    image_reciprocated[image_reciprocated > max_val] = max_val
    image_reciprocated = (image_reciprocated - image_reciprocated.min()) / (image_reciprocated.max() - image_reciprocated.min() + epsilon)
    image_reciprocated = (image_reciprocated * 255).astype(np.uint8)
    return image_reciprocated

def terneary_thresholding_transform(
    image : np.ndarray[np.uint8],
    quantiles : tuple[int,int,int]= [0.15,0.5,0.75],
    mask_color : tuple[
            Literal['max', 'min'],
            Literal['max' , 'min' , 'med'],
            Literal['max','min']
        ] = ['min','none','max']
)->np.ndarray[np.uint8]:
    """
    quantiles[0] is the lower bound of percentile values below this will be painted black
    quantiles[1] is the median of the values for the most cases. Other pixel values will be painted with value of this percentile
    quantiles[2] is the upper bound of the percentile value above this will be painted white

    mask_color[0,1,2] is the color of the mask 0 for values:
    -mask_color[0] for below quantiles[0]%ile
    -mask_color[2] for above quantiles[1]%ile
    -mask_color[1] for between above two mentined percentiles

    # Not necessaryt to change these values you can get same results later through boolean operations as there are only 3 colors
    it can take values 'min' if you want to paint with quantile[0]%ile value percentile, 'max' if you want to paint with quantiles[2]%ile value
    """
    image_reciprocated = image.astype(np.float64)
    image_reciprocated = (image_reciprocated - image_reciprocated.min()) / (image_reciprocated.max() - image_reciprocated.min() + 0.01)
    min_val,med_val,max_val = np.quantile(image_reciprocated.flatten(),quantiles)
    image_reciprocated[image_reciprocated < min_val] = max_val if mask_color[0] == 'max' else min_val
    if mask_color[1] != 'none':
        image_reciprocated[(image_reciprocated > min_val)  &  (image_reciprocated < max_val)] =  max_val if mask_color[1] == 'max' else (med_val if mask_color[1] == 'med' else min_val)
    image_reciprocated[image_reciprocated > max_val] = max_val =  max_val if mask_color[2] == 'max' else min_val
    image_reciprocated = (image_reciprocated - min_val) / (max_val - min_val + 0.001)

    image_reciprocated = (image_reciprocated * 255).astype(np.uint8)
    return image_reciprocated



def draw_bounding_boxes_from_rects(
    image : np.ndarray[np.uint8],
    bboxes : np.ndarray[np.int32],
    shade = 0,
    thickness = 2,
    inplace = False,
):
    """
    Draws the rectangular bounding boxes given an bboxes array of shape (n,5)
    this type of bbox is returned by cv2.connected_components_with_stats where 5th value is area of the connected component

    shade : color of the bounding box, can be color or grayscale value
    thickness : thickness of the bounding box
    inplace : if you want to draw bounding boxes directly on the image, if you don't want to copy the image to save memory
    """
    output = None
    if inplace:
        output = image
    else:
        output = image.copy()


    for stats in bboxes:
        x = stats[cv2.CC_STAT_LEFT]
        y = stats[cv2.CC_STAT_TOP]
        w = stats[cv2.CC_STAT_WIDTH]
        h = stats[cv2.CC_STAT_HEIGHT]
        output = cv2.rectangle(output,(x,y),(x + w, y + h),shade,thickness)
    return output

def detect_bound_islands_with_stats(
    mask_image : np.ndarray[np.uint8],
    ret_labels = False,
    min_circularity_threshold = None,
    min_area_threshold = None,
    max_area_threshold = None,
    min_bbox_area_threshold = None,
    max_bbox_area_threshold = None,
)  -> dict:
    """
    mask_image is the image of blobs on which islands and their stats are to be detected
    """
    output = cv2.connectedComponentsWithStats(mask_image,)
    (numLabels, labels, stats, centroids) = output
    labels_df = pd.DataFrame({'label_id' : list(range(1,numLabels))}).astype(np.int32)
    stats_df = pd.DataFrame(stats[1:],columns=['coords_x', 'coords_y','width','height','area']).astype(np.float32)
    stats_df['bbox_area' ] = stats_df['height'] * stats_df['width']
    centroids_df = pd.DataFrame(centroids[1:],columns=['c_x','c_y']).astype(np.float32)
    combined_df = pd.concat([labels_df,stats_df,centroids_df], axis = 1)
    combined_df['circularity'] = np.nan
    array_mask = pd.Series(np.full((numLabels - 1,),True))
    if min_bbox_area_threshold is not None :
        array_mask = array_mask & (combined_df['bbox_area'] >= min_bbox_area_threshold)
    if max_bbox_area_threshold is not None :
        array_mask = array_mask & (combined_df['bbox_area'] <= max_bbox_area_threshold)
    if min_area_threshold is not None :
        array_mask = array_mask & (combined_df['area'] >= min_area_threshold)
    if max_area_threshold is not None :
        array_mask = array_mask & (combined_df['area'] <= max_area_threshold)

    for i,val in list(combined_df.loc[array_mask,'label_id'].items()):
        # extract the connected component statistics and centroid for

        if min_circularity_threshold is not None:
            circularity = calculate_circularity_of_boolean_mask((labels==val))
            if circularity < min_circularity_threshold:
                array_mask[i] = False
            else:
                combined_df.loc[i,'circularity'] = circularity
    combined_df['circularity'] = combined_df['circularity'].astype(np.float32)
    output_dict = {}
    output_dict['df'] = combined_df.loc[array_mask,:].reset_index().copy()
    if ret_labels:
        output_dict['label_image'] = labels
    return output_dict

def combined_transforms(
    image,
    adaptive_intensity_scaling_args = [],
    adaptive_intensity_scaling_kwargs = {'quantiles' : [0.05,0.95]},
    terneary_thresholding_transform_args = [],
    terneary_thresholding_transform_kwargs = {'quantiles' : [0.20,0.5,0.80], 'mask_color' : ['min','med','max']},
    median_blur_args = [],
    median_blur_kwargs = {'ksize' : 7},
    detect_bound_islands_with_stats_white_args = [],
    detect_bound_islands_with_stats_white_kwargs = {
        'ret_islands':True,
    },
    detect_bound_islands_with_stats_black_args = [],
    detect_bound_islands_with_stats_black_kwargs = {
        'ret_islands':True,
    },
):
    """
    Don't change anything unless you understand the functions used in it. The name of parameters are obvious. kwargs are keyword arguments and args are normal function arguments
    It returns the dictionary of following:
    'coarse_mask' : The mask before the median blur and after terneary thresholding
    'mask' : The mask after mediam blur
    """
    image = adaptive_intensity_scaling(image,*adaptive_intensity_scaling_args,**adaptive_intensity_scaling_kwargs)
    coarse_mask = terneary_thresholding_transform(image,*terneary_thresholding_transform_args, **terneary_thresholding_transform_kwargs)
    mask = cv2.medianBlur(coarse_mask,*median_blur_args,**median_blur_kwargs)
    white_stats = detect_bound_islands_with_stats(
        (mask == mask.max())*np.uint8(255),*detect_bound_islands_with_stats_white_args,**detect_bound_islands_with_stats_white_kwargs
    )
    black_stats = detect_bound_islands_with_stats(
        (mask == mask.min())*np.uint8(255),*detect_bound_islands_with_stats_black_args,**detect_bound_islands_with_stats_black_kwargs
    )
    return {
        'coarse_mask' : coarse_mask,
        'mask' : mask,
        'white_stats_df' : white_stats.get('df'),
        'white_label_image' : white_stats.get('label_image'),
        'black_stats_df' : black_stats.get('df'),
        'black_label_image' : black_stats.get('label_image'),
    }

def show_image(image,figsize = (10,10)):
    plt.figure(figsize = (10,10))
    h,w = image.shape[:2]
    plt.imshow(image.reshape(h,w,-1) + [[[0,0,0]]])
    plt.axis('off')
    plt.show()


def calculate_circularity_of_boolean_mask(boolean_mask, area = None):
    """
    Calculate the circularity of the connected component in a boolean mask using OpenCV.

    Parameters:
    - boolean_mask: np.array, a binary mask where the component is True.

    Returns:
    - float, the circularity of the component.
    """
    # Convert the boolean mask to uint8
    uint8_mask = (boolean_mask * 255).astype(np.uint8)

    # Find contours in the mask
    contours, _ = cv2.findContours(uint8_mask, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

    if len(contours) == 0:
        raise ValueError("No connected component found in the boolean mask.")
    # Assuming a single connected component
    contour = contours[0]

    # Calculate area and perimeter
    if area is None:
        area = cv2.contourArea(contour)
    perimeter = cv2.arcLength(contour, True)

    if perimeter == 0:
        return 0

    # Calculate circularity
    circularity = 4 * np.pi * (area / (perimeter ** 2))
    return circularity

def plot_images_list(image_list, n_cols = 3, max_width = 15):
    h = len(image_list)
    total = h
    fig_x = max_width
    n_rows = math.ceil(total / n_cols)
    fig_y = (max_width / n_cols) * n_rows
    fig,ax = plt.subplots(nrows = n_rows, ncols = n_cols,figsize = (fig_x,fig_y))
    axes = iter(ax.flatten())
    for i in range(h):
        a = next(axes)
        a.imshow(image_list[i].reshape(image_list[i].shape[0], image_list[i].shape[1],-1) + np.array([[[0,0,0]]]))
        a.axis('off')
        a.set_title(f'({i})')
        a.margins(0)

    while True:
        try:
            a = next(axes)
            a.axis('off')
            a.margins(0)
        except StopIteration:
            break
    plt.subplots_adjust(wspace=0)
    plt.show()


def combined_transforms_2(
    image,
    adaptive_intensity_scaling_args = [],
    adaptive_intensity_scaling_kwargs = {'quantiles' : [0.05,0.95]},
    terneary_clustering_thresholding_args = [],
    terneary_clustering_thresholding_kwargs = {},
    median_blur_args = [],
    median_blur_kwargs = {'ksize' : 7},
    detect_bound_islands_with_stats_white_args = [],
    detect_bound_islands_with_stats_white_kwargs = {
        'ret_islands':True,
    },
    detect_bound_islands_with_stats_black_args = [],
    detect_bound_islands_with_stats_black_kwargs = {
        'ret_islands':True,
    },
):
    """
    Don't change anything unless you understand the functions used in it. The name of parameters are obvious. kwargs are keyword arguments and args are normal function arguments
    It returns the dictionary of following:
    'coarse_mask' : The mask before the median blur and after terneary thresholding
    'mask' : The mask after mediam blur
    """
    image = adaptive_intensity_scaling(image,*adaptive_intensity_scaling_args,**adaptive_intensity_scaling_kwargs)
    coarse_mask = terneary_clustering_thresholding(image,*terneary_clustering_thresholding_args, **terneary_clustering_thresholding_kwargs)
    mask = cv2.medianBlur(coarse_mask,*median_blur_args,**median_blur_kwargs)
    white_stats = detect_bound_islands_with_stats(
        (mask == mask.max())*np.uint8(255),*detect_bound_islands_with_stats_white_args,**detect_bound_islands_with_stats_white_kwargs
    )
    black_stats = detect_bound_islands_with_stats(
        (mask == mask.min())*np.uint8(255),*detect_bound_islands_with_stats_black_args,**detect_bound_islands_with_stats_black_kwargs
    )
    return {
        'coarse_mask' : coarse_mask,
        'mask' : mask,
        'white_stats_df' : white_stats.get('df'),
        'white_label_image' : white_stats.get('label_image'),
        'black_stats_df' : black_stats.get('df'),
        'black_label_image' : black_stats.get('label_image'),
    }



def erosion_and_dialation_using_stream(img,stream = "",iterations = 1,kernel = (3,3)):
    output = img.copy();
    for i in stream:
        if i == 'D':
            output = cv2.dilate(output, kernel, iterations=iterations)
        else:
            output = cv2.erode(output, kernel, iterations=iterations)
    return output

"""# Image Visualizations"""

img_slice = extract_slice(
    file_path='/kaggle/input/isro-chandrayan-ohrc-dataset/other/dataset/6/dataset/ch2_ohr_ncp_20200824T0806596861_d_img_d18/data/calibrated/20200824/ch2_ohr_ncp_20200824T0806596861_d_img_d18.img',
    width = 12000,
    height = 90148,
    x = 10000,
    y = 10000,
    slice_width = 1200,
    slice_height = 1200,
)
print(img_slice.shape)

img_slice = cv2.resize(img_slice,(1200,1200))

show_img(img_slice)

adapted = adaptive_intensity_scaling(img_slice,quantiles = [0.05,0.95])

plt.hist(adapted.flatten(), bins = 100)
plt.show()

show_img((adapted))

cluster_result = terneary_clustering_thresholding(adapted)

show_img((cluster_result))

final = combined_transforms(
    img_slice,
    median_blur_kwargs={
        'ksize' : 3
    },
    detect_bound_islands_with_stats_black_kwargs = {
        'ret_labels':True,
        'min_circularity_threshold' : 0.6,
        'min_bbox_area_threshold' : 200,
        'max_bbox_area_threshold' : 360000,
    },
    detect_bound_islands_with_stats_white_kwargs={
        'ret_labels':True,
        'min_circularity_threshold' : 0.6,
    }
)

show_img(final['mask'])

final.keys()

final['black_stats_df']

final_w_bbox = draw_bounding_boxes_from_rects(
    img_slice,
    final['black_stats_df'][['coords_x','coords_y','width','height']].to_numpy(dtype = np.int32)
)

show_image(final_w_bbox)

final['white_coords']

show_img(final['mask'])

plot_images_list([
    final['mask'],
    final['black_bounding_box'],
],max_width = 22,n_cols = 2)

mask = final['mask']
show_img(((mask == mask.min()))*np.uint8(255))

def morph_shape(val):
    if val == 0:
        return cv2.MORPH_RECT
    elif val == 1:
        return cv2.MORPH_CROSS
    elif val == 2:
        return cv2.MORPH_ELLIPSE
erosion_shape = morph_shape(2)
erosion_size = 10

element = cv2.getStructuringElement(erosion_shape, (2 * erosion_size + 1, 2 * erosion_size + 1),
 (erosion_size, erosion_size))

def get_elliptic_element(erosion_size):
    return cv2.getStructuringElement(cv2.MORPH_ELLIPSE , (2 * erosion_size + 1, 2 * erosion_size + 1),(erosion_size, erosion_size))

mask = final['mask']

eroded_dialated = ((mask == mask.min()))*np.uint8(255)
# eroded_dialated = erosion_and_dialation_using_stream(((mask == mask.min()) | (mask == mask.max()))*np.uint8(255), 'ED',1, element)
eroded_dialated = erosion_and_dialation_using_stream(eroded_dialated, 'E',1, get_elliptic_element(3))
eroded_dialated = erosion_and_dialation_using_stream(eroded_dialated, 'D',1, get_elliptic_element(3))


show_img(eroded_dialated)

final_after_erosion = combined_transforms(
    eroded_dialated,
    median_blur_kwargs={
        'ksize' : 7
    },
    detect_bound_islands_with_stats_black_kwargs = {
        'ret_labels' : True,
        'min_bbox_area_threshold':400,
        'min_area_threshold' : 123,
        'min_circularity_threshold' : 0.5,
    },
    detect_bound_islands_with_stats_white_kwargs={
        'ret_labels' : True,
        'min_area_threshold':0,
        'min_circularity_threshold' : 0.5,
    }
)

final_after_erosion_data = detect_bound_islands_with_stats(
    mask_image=eroded_dialated,
    **{
        'ret_labels' : False,
        'min_bbox_area_threshold':200,
        'min_circularity_threshold' : 0.6,
    }

)

final_w_bbox = draw_bounding_boxes_from_rects(
    img_slice,
    final_after_erosion_data['df'][['coords_x','coords_y','width','height']].to_numpy(dtype = np.int32)
)

show_img(final_w_bbox)

params = cv2.SimpleBlobDetector_Params()
params.filterByColor = True
params.blobColor = 255

# params.minDistBetweenBlobs = 1
# params.minRepeatability = 1

# params.filterByArea = True
# params.minArea = 100
# params.maxArea = 3000

# Set Circularity filtering parameters
# params.filterByCircularity = True
# params.minCircularity = 0.3

# Set Convexity filtering parameters
# params.filterByConvexity = True
# params.minConvexity = 0.9

# # Set inertia filtering parameters
# params.filterByInertia = True
# params.minInertiaRatio = 0.5

# Create a detector with the parameters
detector = cv2.SimpleBlobDetector_create(params)

# Detect blobs
keypoints = detector.detect((final['mask']  == final['mask'].min())*np.uint8(255))

# Draw blobs on our image as red circles

blobs = cv2.drawKeypoints(img_slice[:600,600:1200], keypoints, np.array([]), (0, 0, 255),
                          cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
                         )

keypoints[0].class_id

show_img(blobs)

params = cv2.SimpleBlobDetector_Params()
params.filterByColor = True
params.blobColor = 255

params.minDistBetweenBlobs = 1
params.minRepeatability = 1

# params.filterByArea = True
# params.minArea = 100
# params.maxArea = 3000

# Set Circularity filtering parameters
# params.filterByCircularity = True
# params.minCircularity = 0.3

# Set Convexity filtering parameters
# params.filterByConvexity = True
# params.minConvexity = 0.9

# # Set inertia filtering parameters
# params.filterByInertia = True
# params.minInertiaRatio = 0.5

# Create a detector with the parameters
detector = cv2.SimpleBlobDetector_create(params)

# Detect blobs
keypoints = detector.detect((blurred_thresholded  == blurred_thresholded.min())*np.uint8(255))

# Draw blobs on our image as red circles

blobs = cv2.drawKeypoints(img_slice, keypoints, np.array([]), (0, 0, 255),
#                           cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS
                         )

show_img(blobs,(5,5))

"""## Downsampled_image"""

img = cv2.imread(
    '/kaggle/input/isro-chandrayan-ohrc-dataset/other/dataset/6/dataset/ch2_ohr_ncp_20190906T1246532096_d_img_d18/browse/calibrated/20190906/ch2_ohr_ncp_20190906T1246532096_b_brw_d18.png',
#     '/kaggle/input/chandrayan-ohrc-dataset/ch2_ohr_ncp_20240425T1406019344_d_img_d18/browse/calibrated/20240425/ch2_ohr_ncp_20240425T1406019344_b_brw_d18.png'
#     '/kaggle/input/chandrayan-ohrc-dataset/ch2_ohr_ncp_20240406T0052354542_d_img_d18/browse/calibrated/20240406/ch2_ohr_ncp_20240406T0052354542_b_brw_d18.png'
)
img.shape

h,w,img_slices_morph_df,img_slices = slice_and_plot(morph_img,150,150)

plot_images(img_slices_morph_df.iloc[:,:].to_numpy(),n_cols = 3)

"""# Slicing from a sliding window"""

# base_dirs = [
#     '/kaggle/input/isro-chandrayan-ohrc-dataset/other/dataset/6',
#     '/kaggle/input/isro-chandrayan-ohrc-dataset/other/dataset/7',
#     '/kaggle/input/isro-chandrayan-ohrc-dataset/other/dataset/14',
# ]

# image_paths  = []
# for base_dir in base_dirs:
#     image_paths.extend(get_paths_by_re(base_dir,'dataset/**/browse/calibrated/**/*.png'))
# print(len(image_paths))
# pprint(image_paths[:5])

image_paths = [
    '/kaggle/input/isro-chandrayan-ohrc-dataset/other/dataset/6/dataset/ch2_ohr_ncp_20190906T1246532096_d_img_d18/browse/calibrated/20190906/ch2_ohr_ncp_20190906T1246532096_b_brw_d18.png'
]

df = get_slices_from_image_path_list(
    image_paths[-1:-5:-1],
    1200,
    1200,
    7,
)

THRESHOLD_FOR_COMBINATION = 1024
IOU_THRESHOLD = 0.15
AREA_BIAS = 50
def intersection_area(rects1, rects2):
    """
    Calculate the intersection area of two rectangles.

    Parameters:
    rect1, rect2: NumPy arrays of shape (4,) where
        [x, y, w, h] are the coordinates of the top-left corner and dimensions of the rectangle.

    Returns:
    The area of the intersection of the two rectangles.
    """
    x1 = rects1[:, 0]
    y1 = rects1[:, 1]
    w1 = rects1[:, 2]
    h1 = rects1[:, 3]

    x2 = rects2[:, 0]
    y2 = rects2[:, 1]
    w2 = rects2[:, 2]
    h2 = rects2[:, 3]

    x_inter1 = np.maximum(x1, x2)
    y_inter1 = np.maximum(y1, y2)
    x_inter2 = np.minimum(x1 + w1, x2 + w2)
    y_inter2 = np.minimum(y1 + h1, y2 + h2)

    inter_width = np.maximum(0, x_inter2 - x_inter1)
    inter_height = np.maximum(0, y_inter2 - y_inter1)

    return inter_width * inter_height

def union_area(rects1, rects2):
    """
    Calculate the union area of two sets of rectangles.

    Parameters:
    rects1, rects2: NumPy arrays of shape (n, 4) where
        [x, y, w, h] are the coordinates of the top-left corner and dimensions of the rectangles.

    Returns:
    A NumPy array of shape (n,) containing the union areas.
    """
    inter_area = intersection_area(rects1, rects2)

    rects1_area = rects1[:, 4]
    rects2_area = rects2[:, 4]

    return rects1_area + rects2_area - inter_area

def calculate_iou(rects1, rects2):
    """
    Calculate the Intersection over Union (IoU) of two sets of rectangles.

    Parameters:
    rects1, rects2: NumPy arrays of shape (n, 4) where
        [x, y, w, h] are the coordinates of the top-left corner and dimensions of the rectangles.

    Returns:
    A NumPy array of shape (n,) containing the IoU values.
    """
    inter_area = intersection_area(rects1, rects2)
    union_area_val = union_area(rects1, rects2)

    return inter_area / union_area_val
def get_threshold_iou_bounding_box(rects1, rects2):
    x1 = rects1[:, 0]
    y1 = rects1[:, 1]
    w1 = rects1[:, 2]
    h1 = rects1[:, 3]

    x2 = rects2[:, 0]
    y2 = rects2[:, 1]
    w2 = rects2[:, 2]
    h2 = rects2[:, 3]

    x_min = np.minimum(x1, x2)
    y_min = np.minimum(y1, y2)
    w_max = np.maximum(x1 + w1, x2 + w2) - x_min
    h_max = np.maximum(y1 + h1, y2 + h2) - y_min
    max_dim = np.maximum(w_max,h_max)
    return np.vstack([x_min,y_min,max_dim,max_dim],dtype=np.int32).T

def get_bounding_box_matrix(white_arr : np.ndarray,black_arr:np.ndarray):
    detected_bounds = []
    for black_rect in black_arr:
        desired_areas = np.abs(np.log2((white_arr[:,4] + AREA_BIAS) / (black_rect[4] + AREA_BIAS)))
        desired_areas = desired_areas <= THRESHOLD_FOR_COMBINATION
        white_rect = white_arr[desired_areas]
        ious = calculate_iou(black_rect.reshape(1,-1),white_rect)
        detected_bounds.append(
            get_threshold_iou_bounding_box(
                black_rect.reshape(1,-1),
                white_rect[ious >= IOU_THRESHOLD]
            )
        )
    return np.vstack(detected_bounds)

def draw_bounding_boxes(img = None, img_path = None, rectangles = None):
        image = None
        if img is not None:
            image = img.copy()
            image = image.reshape(image.shape[0],image.shape[1],-1) + np.zeros((1,1,3), dtype ='uint8')
        elif img_path is not None:
            image = cv2.imread(img_path)
        for (x, y, w, h) in rectangles:
            dim = max(w,h)
            top_left = (x, y)
            bottom_right = (x + dim, y + dim)
            color = (0, 255, 0)  # Green color in BGR
            thickness = 2  # Thickness of the bounding box
            image = cv2.rectangle(image, top_left, bottom_right, color, thickness)
        return image

def slice_bounding_boxes(image, bounding_boxes):
    """
    Slices rectangular regions from an image based on bounding boxes.

    Parameters:
    - image: np.ndarray, input image.
    - bounding_boxes: list of tuples or np.array of shape (n, 4), where each bounding box is (x, y, width, height).

    Returns:
    - slices: list of np.array, list of sliced image regions.
    """
    # Load the image

    slices = []

    # Iterate through the bounding boxes and slice the regions from the image
    for (x, y, w, h) in bounding_boxes:
        # Define the region of interest (ROI)
        roi = image[y:y + h, x:x + w]
        slices.append(roi)

    return slices

bboxes = get_bounding_box_matrix(
    df['white_df'][0].to_numpy(),
    df['black_df'][0].to_numpy(),
)

plot_images_list([
    df['white_bounding_box'][0],
    df['black_bounding_box'][0],
], n_cols = 2,max_width = 15)

img_slice = img[:1200,:1200,0]

show_img(draw_bounding_boxes(img = img_slice,rectangles = bboxes))

bbox_img_slices = slice_bounding_boxes(img[:1200,:1200],bboxes)

plot_images_list(bbox_img_slices)

import cv2
import numpy as np
import torch
import torchvision.transforms as transforms
from torchvision import models

def preprocess_image(image, target_size):
    """
    Resize or pad the image to the target size based on whether it's square or not.

    Parameters:
    - image: np.array, the input image.
    - target_size: tuple of (height, width), the desired output size.

    Returns:
    - np.array, the resized or padded image.
    """
    height, width = target_size
    h, w, _ = image.shape

    if h == w:
        # Resize if the image is square
        image = cv2.resize(image, (width, height))
    else:
        # Pad then resize if the image is not square
        if h > w:
            # Pad width
            pad_left = (h - w) // 2
            pad_right = h - w - pad_left
            padded_image = cv2.copyMakeBorder(image, 0, 0, pad_left, pad_right, cv2.BORDER_CONSTANT, value=[0, 0, 0])
        else:
            # Pad height
            pad_top = (w - h) // 2
            pad_bottom = w - h - pad_top
            padded_image = cv2.copyMakeBorder(image, pad_top, pad_bottom, 0, 0, cv2.BORDER_CONSTANT, value=[0, 0, 0])

        # Resize to the target size after padding
        image = cv2.resize(padded_image, (width, height))

    return image

def get_image_embeddings(images, model, preprocess):
    """
    Get embeddings for a list of images using a pre-trained ResNet model.

    Parameters:
    - images: list of np.array, list of images to process.
    - model: PyTorch model, the ResNet model.
    - preprocess: torchvision.transforms.Compose, preprocessing pipeline.

    Returns:
    - list of np.array, list of image embeddings.
    """
    model.eval()  # Set the model to evaluation mode
    embeddings = []

    with torch.no_grad():
        for image in images:
            # Preprocess the image
            input_tensor = preprocess(image)
            input_batch = input_tensor.unsqueeze(0)  # Create a mini-batch as expected by the model

            # Get the embedding
            output = model(input_batch)
            embeddings.append(output.squeeze().numpy())

    return embeddings

resnet_model = models.resnet50(pretrained=True)
preprocess = transforms.Compose([
    transforms.ToPILImage(),  # Convert the image to PIL format
    transforms.ToTensor(),  # Convert the image to a tensor
    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),  # Normalize
])


target_size = (224, 224)  # ResNet input size
processed_images = [preprocess_image(img, target_size) for img in bbox_img_slices]

# Get embeddings
embeddings = get_image_embeddings(processed_images, resnet_model, preprocess)

embeddings_arr = np.vstack(embeddings)

from sklearn.metrics.pairwise import cosine_similarity

sim_mat = cosine_similarity(embeddings_arr,embeddings_arr)

plt.imshow(sim_mat)

embeddings_arr.shape

df.columns

df['white_df'][0].to_numpy()

df['white_df'][0]['white_x']

img_gray = cv2.imread(df.iloc[1]['img_path'],cv2.IMREAD_GRAYSCALE)[
    ser['y_start'] : ser['y_start'] + ser['slice_weight'],
    ser['x_start'] : ser['x_start'] + ser['slice_height']
]

denoised = cv2.fastNlMeansDenoising(img_gray, None, h=10, templateWindowSize=7, searchWindowSize=21)

combined_transforms(denoised,7)['mask']

morph = morphological_transform()

show_img(morph)

ellipse_img = fit_ellipse(img_gray,morph)

show_img(ellipse_img)

show_img(combined_transforms(denoised,7)['black_bounding_box'])

df.iloc[1]['img_path']

df.iloc[1]

show_img(df['black_bounding_box'][1])

show_img(df['white_bounding_box'][1])

show_img(df['mask'][1])

"""# Model Training"""

!pip install -q ultralytics

import os
import cv2
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
import seaborn as sns
from sklearn.model_selection import train_test_split
from ultralytics import YOLO

train_img_path = "/kaggle/input/martianlunar-crater-detection-dataset/craters/train/images/"
train_lbl_path = "/kaggle/input/martianlunar-crater-detection-dataset/craters/train/labels/"
valid_img_path = "/kaggle/input/martianlunar-crater-detection-dataset/craters/valid/images/"
valid_lbl_path = "/kaggle/input/martianlunar-crater-detection-dataset/craters/valid/labels/"
test_img_path = "/kaggle/input/martianlunar-crater-detection-dataset/craters/test/images/"
test_lbl_path = "/kaggle/input/martianlunar-crater-detection-dataset/craters/test/labels/"
model_path = "/kaggle/input/martianlunar-crater-detection-dataset/best.pt"
data_yaml_path = "/kaggle/working/data.yaml"

def load_labels(label_path):
    label_files = os.listdir(label_path)
    data = []
    classes = set()
    for file in label_files:
        with open(os.path.join(label_path, file), 'r') as f:
            lines = f.readlines()
            for line in lines:
                parts = list(map(float, line.strip().split()))
                data.append([file, *parts])
                classes.add(int(parts[0]))
    df = pd.DataFrame(data, columns=['file', 'class', 'x_center', 'y_center', 'width', 'height'])
    return df, sorted(classes)

train_labels, train_classes = load_labels(train_lbl_path)
valid_labels, valid_classes = load_labels(valid_lbl_path)
test_labels, test_classes = load_labels(test_lbl_path)

!kaggle kernels output muhammadfaizan65/martian-lunar-crater-detection-yolov8 -p ./copied_output

# Visualize sample detections
def visualize_detections(model, image_path, n_samples=10):
    image_files = os.listdir(image_path)[:n_samples]
    for img_file in image_files:
        img_path = os.path.join(image_path, img_file)
        img = cv2.imread(img_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
        print(img_path)
        results = model(img_path)

        fig, ax = plt.subplots(1, 1, figsize=(10, 10))
        ax.imshow(img)

        for result in results[0].boxes:
            x_min, y_min, x_max, y_max = result.xyxy[0].tolist()
            conf = result.conf[0].item()
            rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min, edgecolor='red', facecolor='none', linewidth=2)
            ax.add_patch(rect)
            ax.text(x_min, y_min, f'{conf:.2f}', bbox=dict(facecolor='yellow', alpha=0.5))

        plt.title(f'Detection in: {img_file}')
        plt.axis('off')
        plt.show()

import torch

device = torch.device('cpu')
model = YOLO('yolov8n.pt')
model.load('/kaggle/working/copied_output/best_model.pt')

img_dir = 'test_images'
os.makedirs(img_dir,exist_ok=True)

image_number = 1
for row in img_slices:
    for img in row:
        if img.shape[0] == img.shape[1]:
            i = Image.fromarray(img.astype('uint8'))
            i.save(os.path.join(img_dir,f'{image_number:05d}.png'))
            image_number += 1

image_paths = [os.path.join(img_dir,file) for file in os.listdir(img_dir)]

visualize_detections(model, img_dir)

"""# Detectron 2 Implementation"""

import json

!wget https://github.com/matterport/Mask_RCNN/releases/download/v2.1/balloon_dataset.zip
!unzip balloon_dataset.zip

Image.open('/kaggle/working/balloon/train/6483318883_21facf57cd_b.jpg')

with open('/kaggle/working/balloon/train/via_region_data.json') as f:
    dic = json.load(f)

dic

